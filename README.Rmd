---
output: github_document
always_allow_html: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  message = FALSE, 
  warning = FALSE
)

library(tidyverse)
library(tidyfit)
```

# tidyfit <img src="man/figures/logo.png" align="right" alt="" width="120" />

<!-- badges: start -->
<!-- badges: end -->

`tidyfit` is an `R`-package that facilitates and automates linear regression and classification modeling in a tidy environment. The package includes several methods, such as Lasso, PLS and ElasticNet regressions. `tidyfit` builds on the `tidymodels` suite, but emphasizes automated modeling with a focus on the linear regression and classification coefficients, which are the primary output of `tidyfit`. The objective is to make model fitting, cross validation and model output very simple and standardized across all methods, with the necessary method-specific transformations handled in the background.

## Installation

You can install the development version of tidyfit from [GitHub](https://github.com/) with:

```{r, eval=F}
# install.packages("devtools")
devtools::install_github("jpfitzinger/tidyfit")
library(tidyfit)
```

## Overview

`tidyfit` includes 4 deceptively simple functions:

 - `regress()`
 - `classify()`
 - `m()`
 - `cross_prod()`

`regress` and `classify` perform regression and classification on tidy data. The functions ingest a tibble, prepare input data for the models by splitting groups, partitioning cross validation slices and handling any necessary adjustments and transformations. The data is then passed to the model wrapper `m()` which fits the models:

```{r, eval=F}
regress(
  .data, 
  formula = y ~ x1 + x2, 
  mod1 = m(<args for underlying method>), mod2 = m(), ...,    # Pass multiple model wrappers
  .cv = "vfold", .cv_args = list(), .weights = "weight_col",  # Additional settings
  <some additional arguments>)
)
```

The syntax is identical for `classify`.

`m` is a powerful wrapper for many different regression and classification techniques that can be used with `regress` and `classify`, or stand-alone:

```{r, eval=F}
m(
  <method>,           # e.g. "lm" or "lasso"
  x, y,               # not passed when used within regress or classify
  ...                 # Args passed to underlying method, e.g. stats::lm or glmnet::glmnet
)
```

An important feature of `m()` is that all arguments can be passed as vectors, allowing generalized hyperparameter tuning or scenario analysis for any method:

 - Passing a hyperparameter grid: `m("lasso", lambda = seq(0, 1, by = 0.1))`
 - Different algorithms for robust regression: `m("robust", method = c("M", "MM"))`
 - Logit and Probit models: `m("glm", family = list(binomial(link="logit"), binomial(link="probit")))`

`m()` performs **feature standardization** whenever necessary, it **always includes an intercept** and outputs regression coefficients and additional model information in a tidy format with **statistically comparable results** across all methods.

Finally, predictions are produced using `cross_prod(<fit>, <data>)`. The function takes data groups, different models, different model settings, as well as the response family into account and produces predicted values.

`tidyfit` integrates with `tidymodels`. It uses `dials` to set sensible default hyperparameter grids, it uses `rsample` for cross validation and the prediction results can easily be evaluated using `yardstick`.
  
## Methods implemented in `tidyfit`

```{r, echo = F}
library(kableExtra)
tbl <- data.frame(
  Method = c("OLS", "Generalized least squares", "Robust regression (e.g. Huber loss)", 
             "Quantile regression", "LASSO", "Ridge", "Adaptive LASSO", "ElasticNet",
             "Gradient boosting regression", "Principal components regression", "Partial least squares",
             "Hierarchical feature regression", "Best subset selection", "Bayesian regression", "Pearson correlation"),
  Name = c("lm", "glm", "robust", "quantile", "lasso", "ridge", "adalasso", "enet", "boost", "pcr", "plsr", 
           "hfr", "subset", "bayes", "cor"),
  Package = c("`stats::lm`", "`stats::glm`", "`MASS::rlm`", "`quantreg`", "`glmnet`", "`glmnet`", "`glmnet`", "`glmnet`",
              "`mboost`", "`pls`", "`pls`", "`hfr`", "`bestglm`", "`arm`", "`stats::cor`"),
  Regression = c("yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "n/a"),
  Classification = c("no", "yes", "no", "no", "yes", "yes", "yes", "yes", "yes", "no", "no", "no", "yes", "yes", "n/a")
)

kbl(tbl, align = "lcccc") %>% 
  kable_styling("striped")

```

See `?m` for additional information.

## Example

### Fama-French factor and industry data

`tidyfit` includes a data set of financial factor returns freely available [here](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html). The data set includes monthly industry returns for 10 industries, as well as monthly factor returns for 5 factors:

```{r example}
data <- tidyfit::Factor_Industry_Returns
```

### Fitting a linear regression

Models are fitted using `tidyfit::regress` for regression or `tidyfit::classify` for binomial classification problems. Below a linear regression is fitted using the `tidyfit::m` model wrapper, which standardizes a large number of regression and classification techniques. The date column is masked and the industry column is automatically one-hot encoded:

```{r}
fit <- data %>% 
  regress(Return ~ ., lin_reg = m("lm"), .mask = "Date")
fit
```

Detailed model and hyperparameter information is nested and can be expanded:

```{r}
fit %>% 
  unnest(model_info)
```

Now, instead of fitting a single regression, we need to fit a regression per industry. This is achieved simply by grouping:

```{r}
fit <- data %>% 
  group_by(Industry) %>% 
  regress(Return ~ ., lin_reg = m("lm"), .mask = "Date")
```

Let's plot the factor loadings in a heatmap:

```{r, fig.width=10, fig.height=5, fig.align="center"}
fit %>% 
  ggplot(aes(variable, Industry)) +
  geom_tile(aes(fill = beta)) +
  scale_fill_gradient2(low = "firebrick", high = "forestgreen")
```

### Multiple arguments

One advantage of `tidyfit` is that it allows arguments to be passed to the underlying methods as vectors. For instance, fitting a robust Huber regression (using `MASS::rlm` in the background) instead of a linear regression, it is possible to compare different estimation algorithms by passing a vector of arguments:

```{r}
fit <- data %>% 
  group_by(Industry) %>% 
  regress(Return ~ ., robust_reg = m("robust", method = c("M", "MM")), .mask = "Date")
```

Let's examine the difference in coefficients for a single sector-regression:

```{r}
fit %>% 
  filter(Industry == "Durbl") %>% 
  unnest(model_info) %>% 
  select(Industry, variable, beta, method) %>% 
  spread(method, beta)
```

Passing multiple arguments is also useful when fitting a quantile regression (using `quantreg::rq` in the background):

```{r, fig.width=10, fig.height=5, fig.align="center"}
fit <- data %>% 
  group_by(Industry) %>% 
  regress(Return ~ ., quantile_reg = m("quantile", tau = c(0.1, 0.5, 0.9)), .mask = "Date")

fit %>% 
  filter(Industry == "Durbl") %>% 
  unnest(model_info) %>% 
  select(Industry, variable, beta, tau) %>% 
  mutate(tau = as.factor(tau)) %>% 
  ggplot(aes(variable, beta, color = tau)) +
  geom_point()
```


### Fitting a Lasso regression

Fitting a Lasso regression requires hyperparameter tuning for the penalty `lambda`. This can be done by passing values to `.cv` and `.cv_args`. Cross validation is performed using `rsample`. See `?rsample::vfold_cv`, `?rsample::loo_cv`, `?rsample::initial_split`, `?rsample::initial_time_split` or `?rsample::rolling_origin` to see optional arguments that can be passed to `.cv_args`. A reasonable hyperparameter grid is determined using the `dials` package, or can be passed manually.

```{r, fig.width=10, fig.height=5, fig.align="center"}
fit <- data %>% 
  group_by(Industry) %>% 
  regress(Return ~ ., lasso_reg = m("lasso"), .mask = "Date", 
          .cv = "vfold", .cv_args = list(v = 5))

fit %>% 
  ggplot(aes(variable, Industry)) +
  geom_tile(aes(fill = beta)) +
  scale_fill_gradient2(low = "firebrick", high = "forestgreen")
```

The results do not appear to be different from a linear regression. To compare methods, simply pass multiple models:

```{r, eval=F}
fit <- data %>% 
  group_by(Industry) %>% 
  regress(Return ~ ., lasso_reg = m("lasso"), lin_reg = m("lm"), .mask = "Date", 
          .cv = "vfold", .cv_args = list(v = 5))
```

Of course, a v-fold cross validation is not valid for ordered data. Instead simply set a rolling cross validation. In addition, we can pass a custom grid for `lambda` by adding the argument to `m`. Note also that it is not necessary to specify a model name:

```{r, fig.width=10, fig.height=5, fig.align="center"}
fit <- data %>% 
  group_by(Industry) %>% 
  regress(Return ~ ., m("lasso", lambda = seq(0, 0.4, by = 0.05)), .mask = "Date", 
          .cv = "rolling_origin", 
          .cv_args = list(initial = 60, assess = 24, skip = 24, cumulative = FALSE))

fit %>% 
  ggplot(aes(variable, Industry)) +
  geom_tile(aes(fill = beta)) +
  scale_fill_gradient2(low = "firebrick", high = "forestgreen")
```


### Predicting with an ElasticNet classifier

Let's predict out-of-sample return probabilities:

```{r}
data_train <- data %>% 
  mutate(Return = ifelse(Return > 0, 1, 0)) %>% 
  filter(Date <= 202112)

data_test <- data %>% 
  mutate(Return = ifelse(Return > 0, 1, 0)) %>% 
  filter(Date > 202112)
```

Classification is possible with `tidyfit` using the `classify` function instead of `regress`. This passes a `family = binomial()` argument to the underlying model functions. Note that additional arguments can be specified in the model function that are passed on to the underlying estimator (in this case `glmnet::glmnet`):

```{r}
fit <- data_train %>% 
  group_by(Industry) %>% 
  classify(Return ~ ., enet_clf = m("enet", maxit = 1e+06), .mask = "Date", 
          .cv = "rolling_origin", .cv_args = list(initial = 60, assess = 24, skip = 24, cumulative = FALSE))
```

Predictions can be made for all models using `cross_prod`. As the name indicates, this generates predictions by multiplying data and coefficients (and passing through the respective link function). No model-specific predict methods are used. Predictions automatically apply along the same groups as in the fitted object, and use the response family specified during fitting:

```{r}
pred <- fit %>% 
  cross_prod(data_test) %>% 
  mutate(Predicted = ifelse(prediction > 0.5, 1, 0)) %>% 
  rename(Truth = Return)

# Print a confusion matrix
table(pred$Truth, pred$Predicted)
```

### Parallel computation

`tidyfit` parallelizes cross validation computations using the `future` package in conjunction with `furrr`. Parallel computation can therefore be activated by setting an appropriate plan:

```{r, eval=F}
library(furrr)
plan(multisession(workers = 4))
fit <- data %>% 
  group_by(Industry) %>% 
  regress(Return ~ ., lasso_reg = m("lasso"), .mask = "Date", 
          .cv = "vfold", .cv_args = list(v = 5))
```


### Additional functionality

`tidyfit` makes a few things easier:

 - Methods return statistically comparable outputs. For instance, all covariates are standardized and the coefficients are back-transformed to the original scale. This is not done by all underlying packages (e.g. `pls`, which is used for the PCR and PLSR methods).
 - Hyperparameter grids are set to reasonable starting values. Custom grids can be passed to the model wrapper (e.g. `m("lasso", lambda = seq(0, 1, by = 0.1))`).
 - Hyperparameters can be tuned across all groups or separately within each group by setting the `.tune_each_group` flag.
 - Results for the individual slices can be returned using the `.return_slices` flag. This is particularly useful for rolling window estimation, which can be done by returning the results of a rolling cross validation.



